<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title class="text-center">Towards an Understanding of Our World by GANing Videos in the Wild</title>

    <meta name="description" content="Source code generated using layoutit.com">
    <meta name="author" content="LayoutIt!">

    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">

  </head>
  <body>

    <div class="container-fluid">
		<div class="row">
			<div class="col-md-12">
				<div class="page-header">
					<h1 class="text-center">
						Towards an Understanding of Our World by GANing Videos in the Wild
					</h1>
				</div> 
				<div class="text-center">
					<strong> Bernhard Kratzwald, Zhiwu Huang, Danda Pani Paudel, Luc Van Gool </strong> <br/>
					Computer Vision Lab, ETH Zurich, Switzerland
					<p><a href="https://github.com/bernhard2202/improved-video-gan" class="btn btn-primary btn-sm btn-info" role="button">GitHub</a>
				<a href="https://arxiv.org/abs/1711.11453" class="btn btn-sm btn-success" role="button">Paper</a>
				</div>
				
			</div>
		</div>
		<div class="row">
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
			<div class="col-sm-10 col-md-8 col-lg-6">
				<h1 class="text-center">Abstract</h1>
				<p class="text-justify">
					Existing generative video models work well only for
					videos with a static background. For dynamic scenes, applications
					of these models demand an extra pre-processing
					step of background stabilization. In fact, the task of background
					stabilization may very often prove impossible for
					videos in the wild. To the best of our knowledge, we present
					the first video generation framework that works in the wild,
					without making any assumption on the videosâ€™ content. This
					allows us to avoid the background stabilization step, completely.
					The proposed method also outperforms the state-of-the-
					art methods even when the static background assumption
					is valid. This is achieved by designing a robust onestream
					video generation architecture by exploiting Wasserstein
					GAN frameworks for better convergence. Since the
					proposed architecture is one-stream, which does not formally
					distinguish between fore- and background, it can generate
					and learn from videos with dynamic backgrounds.
					The superiority of our model is demonstrated by successfully
					applying it to three challenging problems: video colorization,
					video inpainting, and future prediction.
				</p>
			</div>
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
		</div>
			<div class="row"><div class="col-md-12"><p> </p> </div></div>

		
		<div class="row">
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
			<div class="col-sm-10 col-md-8 col-lg-6">
				<h3 class="text-center">
					How to reproduce the results 
				</h3>
				<div class="text-aligned">
					<p>
						The code for all the models is available at our
						<a href="https://github.com/bernhard2202/improved-video-gan" class="btn btn-info btn-xs" role="button">GitHub</a>
					</p>
					<p>
						Datasets with static background are available here: 
						<a href="http://carlvondrick.com/tinyvideo/" class="btn btn-info btn-xs" role="button">Generating Videos with Scene Dynamics</a>
					</p>
					<p> 
						Non-static datasets can be aquired with the scripts available 
						<a href="https://github.com/bernhard2202/improved-video-gan/tree/master/extra"  class="btn btn-info btn-xs" role="button">GitHub/extra</a>
					</p>
					<p> 
						If you have any questions left, please feel free to reach out to us: bkratzwald (at) ethz (dot) ch
					</p> 
				
				</div>
			</div>
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
		</div>
		<div class="row"><div class="col-md-12"><p> </p> </div></div>

		<div class="row">
			<div class="col-md-12">
				<h2 class="text-center">
					Generation with static background
				</h2>
				<div class="text-center">

				<img alt="Generation of static golf videos" src="golf.gif" />
		</div>
			</div>
		</div>
		<div class="row"><div class="col-md-12"><p> </p> </div></div>

		<div class="row">
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
			<div class="col-sm-10 col-md-8 col-lg-6">
				<h2 class="text-center">
					Generation with non-static background
				</h2>
				<p class="text-align">We design a stable architecture with
					no prior constraints on the training data. More precisely,
					we design a one-stream generation framework that does
					not formally distinguish between fore- and background, allowing
					us to handle videos with moving backgrounds/cameras.
					Video generation in a single-stream is a fragile task,
					demanding a carefully selected architecture within a stable
					optimization framework. We accomplish such stability
					by exploiting state-of-the-art Wasserstein GAN frameworks
					in the context of video generation.</p>
				<p class="text-center">If you wish to see more results, or results on other datasets, please write an e-mail to bkratzwald (at) ethz (dot) ch<p>
			</div>
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
		</div>
		<div class="row">
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
			
			<div class="col-sm-12 col-md-10 col-lg-8">
				<div class="row">
					<div class="col-sm-4 col-md-4 col-lg-4">
						<p class="text-center">
							<strong>VGAN (one-stream)</strong>
						</p>
						<div class="text-center">
							<img alt="Generation of non-static videos with two-stream VGAN" src="ap1s.gif" />
						</div>
					</div>
					<div class="col-sm-4 col-md-4 col-lg-4">
						<p class="text-center">
							<strong>VGAN (two-stream)</strong>
						</p>
						<div class="text-center">
							<img alt="Generation of non-static videos with two-stream VGAN" src="ap2s.gif" />
						</div>
					</div>
					<div class="col-sm-4 col-md-4 col-lg-4">
						<p class="text-center">
							<strong>iVGAN (ours)</strong>
						</p>
						<div class="text-center">
							<img alt="Generation of non-static videos with our iVGAN" src="ap_ivgan.gif" />
						</div>
					</div>
				</div>
			</div>
			<div class="col-sm-0 col-md-0 col-lg-2"></div>
		</div>

		<div class="row"><div class="col-md-12"><p> </p> </div></div>
		
		<div class="row">
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
			<div class="col-sm-10 col-md-8 col-lg-6">
				<h1 class="text-center">Applications</h1>
				<p class="text-justify">
					In a second step,
we demonstrate the applicability of our model by proposing
a general multifunctional framework dedicated to specific
applications. Our extension augments the generation model
with an auxiliary encoder network and an application specific
loss function. With these modifications, we successfully conduct several experiments for unsupervised end-to-end video colorization,
video inpainting, and future prediction.
				</p>
			</div>
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
		</div>
		
		
		
		<div class="row">
			<h2 class="text-center">
					Video Colorization
			</h2>
			<p class="text-center"> A demo video how this approach can be used to colorize HD-clips can be found on <a class="btn btn-danger btn-xs" role="button"href="https://youtu.be/j-1VQ3uEMCM">YouTube</a> </p>
			
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>gray-scale input</strong>
				</p>
				<div class="text-center">
					<img alt="BW videos" src="bw.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>RGB output</strong>
				</p>
				<div class="text-center">
					<img alt="Colorized outputs" src="col.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
		</div>
		<div class="row">
			<h2 class="text-center">
					Video Inpainting
			</h2>
			<p class="text-center"> For salt and pepper noise: </p>
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>input</strong>
				</p>
				<div class="text-center">

				<img alt="damaged inputs" src="in0.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>output</strong>
				</p>
				<div class="text-center">
					<img alt="recovered outputs" src="out0.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
		</div>
	</div>
		<div class="row">
			<p class="text-center"> For boxes centered in the screen: </p>
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>input</strong>
				</p>
				<div class="text-center">

				<img alt="damaged inputs" src="in.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>output</strong>
				</p>
				<div class="text-center">
					<img alt="recovered outputs" src="out.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
		</div>
	</div>
		<div class="row">
			<p class="text-center"> For boxes at random positions: </p>
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>input</strong>
				</p>
				<div class="text-center">

				<img alt="damaged inputs" src="in1.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>output</strong>
				</p>
				<div class="text-center">
					<img alt="recovered outputs" src="out1.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
		</div>
	</div>

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/scripts.js"></script>
  </body>
</html>