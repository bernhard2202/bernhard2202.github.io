<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <title class="text-center">Towards an Understanding of Our World by GANing Videos in the Wild</title>

    <meta name="description" content="Source code generated using layoutit.com">
    <meta name="author" content="LayoutIt!">

    <link href="css/bootstrap.min.css" rel="stylesheet">
    <link href="css/style.css" rel="stylesheet">

  </head>
  <body>

    <div class="container-fluid">
		<div class="row">
			<div class="col-md-12">
				<div class="page-header">
					<h1 class="text-center">
						Improving Video Generation for Multi-functional Applications
					</h1>
				</div> 
				<div class="text-center">
					<strong> Bernhard Kratzwald, Zhiwu Huang, Danda Pani Paudel, Acharya Dinesh, Luc Van Gool </strong> <br/>
					Computer Vision Lab, ETH Zurich, Switzerland
					<p><a href="https://github.com/bernhard2202/improved-video-gan" class="btn btn-primary btn-sm btn-info" role="button">GitHub</a>
				<a href="https://arxiv.org/abs/1711.11453" class="btn btn-sm btn-success" role="button">Paper</a>
				</div>
				
			</div>
		</div>
		<div class="row">
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
			<div class="col-sm-10 col-md-8 col-lg-6">
				<h1 class="text-center">Abstract</h1>
				<p class="text-justify">
					In this paper, we aim to improve the state-of-the-art video generative adversarial networks (GANs) with a view towards multi-functional applications. Our improved video GAN model does not separate foreground from background nor dynamic from static patterns, but learns to generate the entire video clip conjointly. Our model can thus be trained to generate---and learn from---a broad set of videos with no restriction. This is achieved by designing a robust one-stream video generation architecture with an extension of the state-of-the-art Wasserstein GAN framework that allows for better convergence. The experimental results show that our improved video GAN model outperforms state-of-the-art video generative models on multiple challenging datasets. Furthermore, we demonstrate the superiority of our model by successfully extending it to three challenging problems: video colorization, video inpainting, and future prediction. To the best of our knowledge, this is the first work using GANs to colorize and inpaint video clips.
				</p>
			</div>
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
		</div>
			<div class="row"><div class="col-md-12"><p> </p> </div></div>

		
		<div class="row">
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
			<div class="col-sm-10 col-md-8 col-lg-6">
				<h3 class="text-center">
					How to reproduce the results 
				</h3>
				<div class="text-aligned">
					<p>
						The code for all the models is available at our
						<a href="https://github.com/bernhard2202/improved-video-gan" class="btn btn-info btn-xs" role="button">GitHub</a>
					</p>
					<p>
						Datasets with static background are available here: 
						<a href="http://carlvondrick.com/tinyvideo/" class="btn btn-info btn-xs" role="button">Generating Videos with Scene Dynamics</a>
					</p>
					<p> 
						Non-static datasets can be aquired with the scripts available 
						<a href="https://github.com/bernhard2202/improved-video-gan/tree/master/extra"  class="btn btn-info btn-xs" role="button">GitHub/extra</a>
					</p>
					<p> 
						If you have any questions left, please feel free to reach out to us - we are also happy to share pre-trained models: bkratzwald (at) ethz (dot) ch
					</p> 
				
				</div>
			</div>
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
		</div>
		<div class="row"><div class="col-md-12"><p> </p> </div></div>

		<div class="row">
			<div class="col-md-12">
				<h2 class="text-center">
					Generation with static background
				</h2>
				<div class="text-center">

				<img alt="Generation of static golf videos" src="golf.gif" />
		</div>
			</div>
		</div>
		<div class="row"><div class="col-md-12"><p> </p> </div></div>

		<div class="row">
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
			<div class="col-sm-10 col-md-8 col-lg-6">
				<h2 class="text-center">
					Generation with non-static background
				</h2>
				<p class="text-align">We design a stable architecture with
					no prior constraints on the training data. More precisely,
					we design a one-stream generation framework that does
					not formally distinguish between fore- and background, allowing
					us to handle videos with moving backgrounds/cameras.
					Video generation in a single-stream is a fragile task,
					demanding a carefully selected architecture within a stable
					optimization framework. We accomplish such stability
					by exploiting state-of-the-art Wasserstein GAN frameworks
					in the context of video generation.</p>
				<p class="text-center">If you wish to see more results, or results on other datasets, please write an e-mail to bkratzwald (at) ethz (dot) ch<p>
			</div>
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
		</div>
		<div class="row">
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
			
			<div class="col-sm-12 col-md-10 col-lg-8">
				<div class="row">
					<div class="col-sm-4 col-md-4 col-lg-4">
						<p class="text-center">
							<strong>VGAN (one-stream)</strong>
						</p>
						<div class="text-center">
							<img alt="Generation of non-static videos with two-stream VGAN" src="ap1s.gif" />
						</div>
					</div>
					<div class="col-sm-4 col-md-4 col-lg-4">
						<p class="text-center">
							<strong>VGAN (two-stream)</strong>
						</p>
						<div class="text-center">
							<img alt="Generation of non-static videos with two-stream VGAN" src="ap2s.gif" />
						</div>
					</div>
					<div class="col-sm-4 col-md-4 col-lg-4">
						<p class="text-center">
							<strong>iVGAN (ours)</strong>
						</p>
						<div class="text-center">
							<img alt="Generation of non-static videos with our iVGAN" src="ap_ivgan.gif" />
						</div>
					</div>
				</div>
			</div>
			<div class="col-sm-0 col-md-0 col-lg-2"></div>
		</div>

		<div class="row"><div class="col-md-12"><p> </p> </div></div>
		
		<div class="row">
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
			<div class="col-sm-10 col-md-8 col-lg-6">
				<h1 class="text-center">Applications</h1>
				<p class="text-justify">
					In a second step,
we demonstrate the applicability of our model by proposing
a general multifunctional framework dedicated to specific
applications. Our extension augments the generation model
with an auxiliary encoder network and an application specific
loss function. With these modifications, we successfully conduct several experiments for unsupervised end-to-end video colorization,
video inpainting, and future prediction.
				</p>
			</div>
			<div class="col-sm-1 col-md-2 col-lg-3"></div>
		</div>
		
		
		
		<div class="row">
			<h2 class="text-center">
					Video Colorization
			</h2>
			<p class="text-center"> A demo video how this approach can be used to colorize HD-clips can be found on <a class="btn btn-danger btn-xs" role="button"href="https://youtu.be/j-1VQ3uEMCM">YouTube</a> </p>
			
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>gray-scale input</strong>
				</p>
				<div class="text-center">
					<img alt="BW videos" src="bw.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>RGB output</strong>
				</p>
				<div class="text-center">
					<img alt="Colorized outputs" src="col.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
		</div>
		<div class="row">
			<h2 class="text-center">
					Video Inpainting
			</h2>
			<p class="text-center"> For salt and pepper noise: </p>
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>input</strong>
				</p>
				<div class="text-center">

				<img alt="damaged inputs" src="in0.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>output</strong>
				</p>
				<div class="text-center">
					<img alt="recovered outputs" src="out0.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
		</div>
	</div>
		<div class="row">
			<p class="text-center"> For boxes centered in the screen: </p>
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>input</strong>
				</p>
				<div class="text-center">

				<img alt="damaged inputs" src="in.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>output</strong>
				</p>
				<div class="text-center">
					<img alt="recovered outputs" src="out.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
		</div>
	</div>
		<div class="row">
			<p class="text-center"> For boxes at random positions: </p>
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>input</strong>
				</p>
				<div class="text-center">

				<img alt="damaged inputs" src="in1.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-5 col-lg-4">
				<p class="text-center">
					<strong>output</strong>
				</p>
				<div class="text-center">
					<img alt="recovered outputs" src="out1.gif" />
				</div>
			</div>
			<div class="col-sm-0 col-md-1 col-lg-2"></div>
		</div>
	</div>

    <script src="js/jquery.min.js"></script>
    <script src="js/bootstrap.min.js"></script>
    <script src="js/scripts.js"></script>
  </body>
</html>